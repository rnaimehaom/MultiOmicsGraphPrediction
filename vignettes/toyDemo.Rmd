
---
title: "Ensemble Toy Demo"
author: "Tara Eicher"
date: "2/22/2022"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{MultiOmicsGraphPrediction Toy}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---
# Description
This toy example was created to verify that the proposed ensemble neural network performs as expected.
It includes patients P1-P6, metabolites M1-M6, and genes G1-G11.

In this example, age is the predicted phenotype, and artificial gene and metabolite values have been
created as follows:
- M2 = M1 * age / 2
- M3 = M1 / age
- M4 = -M2 / age
- G4 = -M2 / age
- G6 = -M2 / age
- G7 = M1 / age
- G8 = -M2 / age
- G11 = M1 / age

All other gene and metabolite data were filled in using a random number between
-1 and 1.

We therefore know that age can be predicted using the following predictive models that are in the
form used by our model:
- age = -1 * M2 / G4, i.e. beta0 = 0, beta1 = 0, beta2 = 0, and beta3 = -1
- age = -1 * M2 / G6, i.e. beta0 = 0, beta1 = 0, beta2 = 0, and beta3 = -1
- age = -1 * M2 / G8, i.e. beta0 = 0, beta1 = 0, beta2 = 0, and beta3 = -1
- age = M1 / G7, i.e. beta0 = 0, beta1 = 0, beta2 = 0, and beta3 = 1
- age = M1 / G11, i.e. beta0 = 0, beta1 = 0, beta2 = 0, and beta3 = 1
- age = -2 * M4 / G7, i.e. beta0 = 0, beta1 = 0, beta2 = 0, and beta3 = -0.5
- age = -2 * M4 / G11, i.e. beta0 = 0, beta1 = 0, beta2 = 0, and beta3 = -0.5

The following analyte models also exist but are not predictive of age:
- M3 = G7, i.e. beta0 = 0, beta1 = 1, beta2 = 0, and beta3 = 0
- M3 = G11, i.e. beta0 = 0, beta2 = 1, beta3 = 0, and beta3 = 0

We expect to see the final predictive subgraph comprised of one or more of the predictive models
listed above and not include any of the non-predictive models.

# Set up.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
result_dir <- "~\\Ensemble_toy_vignette_results"
dir.create(result_dir)
```

# Install the packages.
```{r eval = FALSE}
if(!require("devtools")){
  install.packages("devtools")
}
library("devtools")
if(!require("MultiOmicsGraphPrediction")){
  install_github("ncats/MultiOmicsGraphPrediction")
}
library(MultiOmicsGraphPrediction)
if(!require("IntLIM")){
  install_github("ncats/IntLIM")
}
library(IntLIM)
```

# Import Toy data files.
```{r}
dir <- system.file("extdata", package="MultiOmicsGraphPrediction", mustWork=TRUE)
csvfile <- file.path(dir, "toyinput.csv")
inputData <- IntLIM::ReadData(csvfile)
```

# Run IntLIM
We note that the following models are learned as expected:
- M2 = 0 + 0 * G4 + 0 * age + -1 * G4 * age
- M2 = 0 + 0 * G6 + 0 * age + -1 * G6 * age
- M2 = 0 + 0 * G8 + 0 * age + -1 * G8 * age
- M4 = 0 - 0 * G7 - 0 * age -0.5 * G7 * age
- M4 = 0 - 0 * G11 - 0 * age -0.5 * G11 * age
- M1 = 0 + 0 * G7 + 0 * age + 1 * G7 * age
- M1 = 0 + 0 * G11 + 0 * age + 1 * G7 * age
- M3 = 0 + 1 * G7 - 0 * age + 0 * G7 * age
- M3 = 0 + 1 * G11 - 0 * age + 0 * G7 * age
```{r}
myres <- IntLIM::RunIntLim(inputData = inputData,stype="age",
                                 save.covar.pvals = TRUE, 
                                 outcome = 1, 
                                 independent.var.type = 2, 
                                 continuous = TRUE)
View(myres@covariate.coefficients)
```

# Filter Results and Create Co-Regulation Graph
The expected pairs all pass the R^2 threshold.

Other pairs that also pass the threshold are:
(M1, G10)
(M2, [G7, G10, G11])
(M3, [G3, G4, G6, G8, G10])
(M4, [G3, G4, G6, G8, G10])
```{r}
myres.all <- IntLIM::ProcessResults(inputResults = myres, inputData = inputData, 
                                       pvalcutoff = 1, rsquaredCutoff = 0, interactionCoeffPercentile = 0)
myres.r2.density <- density(myres.all$rsquared)
peak.r2 <- myres.r2.density$x[which.max(myres.r2.density$y)]
myres.r2 <- IntLIM::ProcessResults(inputResults = myres, inputData = inputData, 
                                       pvalcutoff = 1, rsquaredCutoff = peak.r2)
# Build graph.
coreg.r2 <- MultiOmicsGraphPrediction::BuildCoRegulationGraph(myres.r2)
MultiOmicsGraphPrediction::PlotCoRegulationGraph(coreg.r2, "", saveInFile = NULL, vertices = NULL)
View(myres.r2)
```

# Run Pairwise Prediction.
We can see that the predictions are exactly correct for (M2, [G4, G6, G8]), 
(M4, [G7, G11]), and (M1, [G7, G11]). Some of the other
pairs also give close predictions. The predictions given by (M3, [G7, G11])
are not correct, which is expected because their relationship does not
depend on age.
```{r}
# Run pairwise prediction.
pred <- MultiOmicsGraphPrediction::RunPairwisePrediction(inputResults = myres.r2, 
                                                         inputData = inputData,
                                                         stype = "age",
                                                         independentVarType = 2,
                                                         outcomeType = 1)
hist(pred, breaks = 100)
print(inputData@sampleMetaData[,"age"])
print(pred)
```

## Project Predictions
```{r plot graph}
# Project graph.
projectedGraph <- MultiOmicsGraphPrediction::ProjectPredictionsOntoGraph(coRegulationGraph = coreg.r2,
                                                                         predictions = pred)
MultiOmicsGraphPrediction::PlotGraphPredictions(graph = projectedGraph,
                                                inputData = inputData,
                                                stype = "age")
```

## Compute Importance Metrics.
In this case, the subspace grouping of patients has little relationship to the phenotype.
However, this is likely due to the analyte values being assigned randomly.

Regarding importance metrics, the exact predictors do not have notably higher PDF scores
than the other predictors. However, the local error importance values are higher for the
exact predictors than the other predictors, as are the interaction p-value importance scores.
```{r importance}
MultiOmicsGraphPrediction::PlotSubspaceClusteringDendrogram(inputData = inputData, eigStep = 1)
MultiOmicsGraphPrediction::PlotSubspaceClusteringHeatmap(inputData = inputData, eigStep = 1)
importance <- MultiOmicsGraphPrediction::GetAllImportanceMetrics(predictions = pred, 
                                                                 stype = "age",
                                                                 inputData = inputData, 
                                                                 metricList = c("pdf", "interactionpval", "interactioncoef", "analytecoef", "localerr"),
                                                                 k = 2, eigStep = 1,
                                                                 modelStats = myres.r2,
                                                                 colIdInd = "databaseId",
                                                                 colIdOut = "databaseId")
hist(importance$pdf)
hist(importance$localerr)
hist(importance$interactionpval)
hist(importance$interactioncoef)
hist(importance$analytecoef)
print(importance)
```

# Look at a predictive model.
## T-Test
The M2-based predictors are grouped together with G4 predictors in neighborhood 1.
The M4-based predictors are grouped together in neighborhood 2, along with the G7 predictors.
The M3-based predictors are grouped together in neighborhood 3.
The M1-based predictors are grouped together with G4 predictors in neighborhood 4.

In neighborhood 1, all predictors are removed except (M2, [G4, G6, G8]), the exact predictors.
In neighborhood 2, all predictors are removed except (M4, [G4, G6, G7, G8, G11]) and (M1, G7),
of which (M4, [G7, G11]) and (M1, G7) are exact predictors.
In neighborhood 3, all predictors are removed except (M3, [G6, G8]) and (M6, G3).
In neighborhood 4, all predictors are removed except (M2, G4).

In the second iteration, neighborhoods 1 and 3 are combined, and nothing is removed
(because neighborhood 3 is a subset of neighborhood 1).
Neighborhoods 2 and 4 do not overlap with any of the other neighborhoods, so they
remain the same and form the new neighborhoods 2 and 3.

In the third iteration, all neighborhoods are combined, and neighborhoods 2 and 3
are removed, leaving only the exact predictors (M2, [G4, G6, G8]).
```{r}
modelInput <- MultiOmicsGraphPrediction::FormatInput(predictionGraphs = projectedGraph, 
                                                     coregulationGraph = coreg.r2,
                                                     inputData = inputData, 
                                                     stype.class = "numeric",
                                                     stype = "age",
                                                     edgeTypeList = c("shared.outcome.analyte", "shared.independent.analyte"),
                                                     importance = importance,
                                                     modelProperties = myres.r2,
                                                     outcome = 1,
                                                     independent.var.type = 2)
modelResults <- MultiOmicsGraphPrediction::InitializeGraphLearningModel(modelInput = modelInput, learningRate = 0.2)
pairsPredAll <- MultiOmicsGraphPrediction::ObtainSubgraphNeighborhoods(modelInput = modelResults@model.input, percentOverlapCutoff = 50)

propagatedGraph <- MultiOmicsGraphPrediction::DoSignificancePropagation(pairs = pairsPredAll, modelResults = modelResults,
                                               verbose = TRUE, makePlots = FALSE, pruningMethod = "error.t.test", includeVarianceTest = TRUE)
print(propagatedGraph)
print(MultiOmicsGraphPrediction::CompositePrediction(modelResults = modelResults, pairs = propagatedGraph[[1]]))
```

# Learn Weights.
Because the final predictors are exact, there is no need to adjust the weights so that
other models contribute more to the final prediction. Therefore, the optimization
converges immediately and returns the same set of predictors as the first iteration.
```{r}
optimalModel <- MultiOmicsGraphPrediction::OptimizeImportanceCombo(modelResults = modelResults,pruningMethod = "error.t.test", includeVarianceTest = TRUE, verbose = TRUE)
```

# Test Performance Using Leave-One-Out Cross-Validation
To perform leave-one-out cross-validation, we need to perform the following steps 6 times
(once for each sample):
1. Split the data into the training set (all samples except the one being left out)
and the testing set (the sample being left out).
2. Run IntLIM, create co-regulation graph, and run individual node prediction on
the training data.
3. Compute metafeatures on the training data.
4. Optimize the training data model.
5. Compute metafeatures on the testing data.
6. Do prediction for the testing data using the computed metafeatures on the testing data
and the subgraph and metafeature weights learned on the training data.
```{r}
```